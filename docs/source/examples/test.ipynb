{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662efe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91b052a3",
   "metadata": {},
   "source": [
    "## pyMAISE Initialization\n",
    "\n",
    "First we initialize pyMAISE with the following 4 parameters:\n",
    "\n",
    "- `verbosity`: 0 $\\rightarrow$ pyMAISE prints no outputs,\n",
    "- `random_state`: None $\\rightarrow$ No random seed is set,\n",
    "- `test_size`: 0.3 $\\rightarrow$ 30% of the data is used for testing,\n",
    "- `num_configs_saved`: 5 $\\rightarrow$ The top 5 hyper-parameter configurations are saved for each model.\n",
    "\n",
    "With pyMAISE initialized we can load the preprocessor for this data set using `load_fp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3928735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 21:57:32.278251: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-10 21:57:35.692809: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-10 21:57:35.773355: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-10 21:57:48.740098: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-10 21:57:51.442519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-10 21:57:51.444322: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import pyMAISE as mai\n",
    "import pytest\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Regression test parameters\n",
    "# Data set parameters\n",
    "num_observations = 150\n",
    "num_features = 4\n",
    "num_outputs = 1\n",
    "\n",
    "expected_models = {\n",
    "    \"dtree\": 1.0,\n",
    "    \"rforest\": 1.0,\n",
    "    \"knn\": 1.0,\n",
    "    \n",
    "}\n",
    "\n",
    "# Expected Model Settings\n",
    "matplotlib_settings = {\n",
    "    \"font.size\": 14,\n",
    "    \"legend.fontsize\": 12,\n",
    "}\n",
    "plus_minus= 0.025\n",
    "\n",
    "settings = { \n",
    "    \"verbosity\": 0,\n",
    "    \"random_state\": 42,\n",
    "    \"test_size\": 0.3,\n",
    "    \"num_configs_saved\": 1,\n",
    "    \"regression\": False,\n",
    "    \"classification\": True,\n",
    "}\n",
    "\n",
    "\n",
    "global_settings = mai.settings.init(settings_changes=settings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = mai.load_iris()\n",
    "data = preprocessor.data_split()\n",
    "\n",
    "model_settings = { \"models\": [\"logistic\", \"dtree\", \"rforest\", \"knn\"],\n",
    "                 }\n",
    "\n",
    "\n",
    "tuning = mai.Tuning(data=data, model_settings=model_settings)\n",
    "\n",
    "grid_search_spaces = {\n",
    "    \"logistic\":{\n",
    "        \"penalty\": ['l1', 'l2', 'elasticnet'],\n",
    "        \"dual\": [True, False],\n",
    "        \"tol\": [0.001],\n",
    "    },\n",
    "    \"dtree\":{\n",
    "        \"max_depth\": [None, 5, 10, 25, 50],\n",
    "        \"max_features\": [None, \"sqrt\", \"log2\"],\n",
    "        \"min_samples_leaf\": [1, 2, 4, 6, 8, 10],\n",
    "        \"min_samples_split\": [2, 4, 6, 8, 10],\n",
    "    },\n",
    "    \"rforest\": {\n",
    "        \"n_estimators\": [50, 100, 150],\n",
    "        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"min_samples_split\": [2, 4, 6],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "                      \n",
    "    },\n",
    "    \"knn\": {\n",
    "        \"n_neighbors\": [1, 2, 4, 6, 8, 10, 14, 17, 20],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"leaf_size\": [1, 5, 10, 15, 20, 30],\n",
    "    },\n",
    "}\n",
    "grid_search_configs = tuning.grid_search(param_spaces=grid_search_spaces, models=grid_search_spaces.keys(), cv=ShuffleSplit(n_splits=1, test_size=0.15, random_state=global_settings.random_state),)\n",
    "\n",
    "postprocessor = mai.PostProcessor(\n",
    "    data=data,\n",
    "    models_list=[grid_search_configs],\n",
    ")\n",
    "\n",
    "for key, value in expected_models.items():\n",
    "    assert postprocessor.metrics(model_type=key)[\"Test Accuracy\"].to_numpy()[0] == pytest.approx(value, plus_minus / value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83806815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Types</th>\n",
       "      <th>Parameter Configurations</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Train Recall</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Train F1</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Recall</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic</td>\n",
       "      <td>{'dual': False, 'penalty': 'l2', 'tol': 0.001}</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dtree</td>\n",
       "      <td>{'max_depth': None, 'max_features': None, 'min...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rforest</td>\n",
       "      <td>{'criterion': 'gini', 'max_features': 'sqrt', ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 20, 'weights'...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Types                           Parameter Configurations  \\\n",
       "0    logistic     {'dual': False, 'penalty': 'l2', 'tol': 0.001}   \n",
       "1       dtree  {'max_depth': None, 'max_features': None, 'min...   \n",
       "2     rforest  {'criterion': 'gini', 'max_features': 'sqrt', ...   \n",
       "3         knn  {'leaf_size': 10, 'n_neighbors': 20, 'weights'...   \n",
       "\n",
       "   Train Accuracy  Train Recall  Train Precision  Train F1  Test Accuracy  \\\n",
       "0        0.961905      0.961905         0.961905  0.961905            1.0   \n",
       "1        1.000000      1.000000         1.000000  1.000000            1.0   \n",
       "2        1.000000      1.000000         1.000000  1.000000            1.0   \n",
       "3        1.000000      1.000000         1.000000  1.000000            1.0   \n",
       "\n",
       "   Test Recall  Test Precision  Test F1  \n",
       "0          1.0             1.0      1.0  \n",
       "1          1.0             1.0      1.0  \n",
       "2          1.0             1.0      1.0  \n",
       "3          1.0             1.0      1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor.metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b24c5",
   "metadata": {},
   "source": [
    "# New NN Object using MIT Reactor Data for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fbdd3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- NEW TEST ---------------\n",
      "{'structural_hyperparameters.dense_input.num_nodes': 12, 'structural_hyperparameters.dense_input.input_dim': 6, 'structural_hyperparameters.dense_input.activation': 'relu', 'structural_hyperparameters.dense.num_nodes': 15, 'structural_hyperparameters.dense.shape': None, 'structural_hyperparameters.dense.activation': 'relu', 'structural_hyperparameters.dense_output.num_nodes': 118, 'structural_hyperparameters.dense_output.shape': 22, 'structural_hyperparameters.dense_output.activation': 'relu'}\n",
      "--------- NEW TEST ---------------\n",
      "--Checking Initialization--\n",
      "opt =  adam\n",
      "loss =  mean_absolute_error\n",
      "adam\n",
      "mean_absolute_error\n"
     ]
    }
   ],
   "source": [
    "import pyMAISE as mai\n",
    "from pyMAISE.methods import *\n",
    "import pytest\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import keras_tuner\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "from kerastuner import HyperModel\n",
    "import flatdict\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "global_settings = mai.settings.init({\"regression\": True})\n",
    "\n",
    "preprocessor = mai.load_MITR()\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = preprocessor.data_split()\n",
    "\n",
    "\n",
    "print(\"--------- NEW TEST ---------------\")\n",
    "optimizer_hyperparameters = {\n",
    "    # Optimizer\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": 0.0099,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 27,\n",
    "    \"loss\": \"mean_absolute_error\",\n",
    "    \"metrics\": [\"mean_absolute_error\"],\n",
    "    \"warm_start\": True,\n",
    "    \"jit_compile\": False,\n",
    "    \n",
    "}\n",
    "\n",
    "structural_hyperparameters = {\n",
    "    \"structural_hyperparameters\":{\n",
    "        \"dense_input\": {\n",
    "            \"num_nodes\": 12, \n",
    "            \"input_dim\": preprocessor.inputs.shape[1],\n",
    "            \"activation\": 'relu',\n",
    "            },\n",
    "        \"dense\": {\n",
    "            \"num_nodes\": 15,\n",
    "            \"shape\": None,\n",
    "            \"activation\": 'relu',\n",
    "        },\n",
    "        \"dense_output\": {\n",
    "            \"num_nodes\": 118,\n",
    "            \"shape\": preprocessor.outputs.shape[1],\n",
    "            \"activation\": 'relu',\n",
    "        },\n",
    "    },\n",
    "    \n",
    "}\n",
    "# An example that we ould do to flatten the dictionary for the potential issue you said about milti dim dictionary\n",
    "d =  flatdict.FlatDict(structural_hyperparameters, delimiter='.')\n",
    "print(d)\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# Using the Baysian Search I saw\n",
    "\n",
    "print(\"--------- NEW TEST ---------------\")\n",
    "\n",
    "hp = keras_tuner.HyperParameters()\n",
    "\n",
    "\n",
    "optimizer_hyperparameters = {\n",
    "    # Optimizer\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": 1e-4, # hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, default=1e-3):w\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 27,\n",
    "    \"loss\": \"mean_absolute_error\",\n",
    "    \"metrics\": [\"mean_absolute_error\"],\n",
    "    \"warm_start\": True,\n",
    "    \"jit_compile\": False,\n",
    "    \n",
    "}\n",
    "\n",
    "structural_hyperparameters = {\n",
    "    \"structural_hyperparameters\":{\n",
    "        \"dense_input\": {\n",
    "            \"num_nodes\": 100, # hp.Int('units1', min_value=50, max_value=350, step=50), \n",
    "            \"input_dim\": preprocessor.inputs.shape[1],\n",
    "            \"activation\": 'relu',\n",
    "            },\n",
    "        \"dense\": {\n",
    "            \"num_nodes\": [50, 100, 5], # hp.Int('units2', min_value=50, max_value=100, step=25),\n",
    "            \"input_dim\": None,\n",
    "            \"activation\": 'relu',\n",
    "        },\n",
    "        \"dense_output\": {\n",
    "            \"num_nodes\":  preprocessor.outputs.shape[1], # hp.Int('units3', min_value=50, max_value=100, step=25),\n",
    "            \"input_dim\": None,\n",
    "            \"activation\": 'relu',\n",
    "        },\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "model_settings = {\n",
    "    \"models\": [\"linear\", \"lasso\", \"dtree\", \"knn\", \"rforest\", \"nn_new\"],\n",
    "    \"nn_new\": [structural_hyperparameters, optimizer_hyperparameters],\n",
    "}\n",
    "\n",
    "\n",
    "build_model = nnHyperModel(model_settings[\"nn_new\"][0], model_settings[\"nn_new\"][1])\n",
    "print(build_model.optimizer)\n",
    "print(build_model.loss)\n",
    "# print(build_model.loss)\n",
    "# tuner = kt.BayesianOptimization(\n",
    "#         build_model,\n",
    "#         objective=\"mean_absolute_error\",\n",
    "#         max_trials=10,\n",
    "#         seed=42)\n",
    "# print(\"[INFO] performing hyperparameter search...\")\n",
    "# tuner.search(\n",
    "#     x=xtrain,\n",
    "#     y=ytrain,\n",
    "#     validation_data=(xtest, ytest),\n",
    "#     batch_size=50,\n",
    "#     epochs=10,\n",
    "# )\n",
    "# # grab the best hyperparameters\n",
    "# bestHP = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "# print(\"[INFO] optimal number of units in dense layer: {}\".format(\n",
    "# \tbestHP.get(\"Units\")))\n",
    "\n",
    "# # build the best model and train it\n",
    "# print(\"[INFO] training the best model...\")\n",
    "# model = tuner.hypermodel.build(bestHP)\n",
    "# H = model.fit(x=xtrain, y=ytrain,\n",
    "# validation_data=(xtest, ytest), batch_size=32,\n",
    "# epochs=50, verbose=1)\n",
    "# # evaluate the network\n",
    "# print(\"[INFO] evaluating network...\")\n",
    "# predictions = model.predict(x=xtest, batch_size=32)\n",
    "# print(r2_score(ytest, predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c83b4a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer =  mean_absolute_error\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"optimizer = \", build_model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de1ec2",
   "metadata": {},
   "source": [
    "# Integrating with current tuning class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc497b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 15:25:36.947793: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-24 15:25:55.689365: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-24 15:25:56.491257: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-24 15:26:23.239627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pyMAISE as mai\n",
    "from pyMAISE.methods import *\n",
    "import pytest\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import keras_tuner\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "from kerastuner import HyperModel\n",
    "import flatdict\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "global_settings = mai.settings.init({\"regression\": True})\n",
    "preprocessor = mai.load_MITR()\n",
    "data = preprocessor.min_max_scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57520a1e",
   "metadata": {},
   "source": [
    "# Model Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e867d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Checking Initialization--\n",
      "opt =  None\n",
      "loss =  None\n",
      "model =  nn_new <pyMAISE.methods._nn_general.nnHyperModel object at 0x7fbeec0c3400>\n",
      "Checking if model is initialized\n",
      "optimizer =  None\n",
      "loss =  None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "structural_hyperparameters = {\n",
    "    \"structural_hyperparameters\":{\n",
    "        \"dense_input\": {\n",
    "            \"num_nodes\": 100, # hp.Int('units1', min_value=50, max_value=350, step=50), \n",
    "            \"input_dim\": preprocessor.inputs.shape[1],\n",
    "            \"activation\": 'relu',\n",
    "            },\n",
    "        \"dense\": {\n",
    "            \"num_nodes\": [50, 100, 5], # hp.Int('units2', min_value=50, max_value=100, step=25),\n",
    "            \"input_dim\": None,\n",
    "            \"activation\": 'relu',\n",
    "        },\n",
    "        \"dense_output\": {\n",
    "            \"num_nodes\":  preprocessor.outputs.shape[1], # hp.Int('units3', min_value=50, max_value=100, step=25),\n",
    "            \"input_dim\": None,\n",
    "            \"activation\": 'relu',\n",
    "        },\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "optimizer_hyperparameters = {\"optimizer_hyperparameters\":{\n",
    "                                    # Optimizer\n",
    "                                    \"optimizer\": \"adam\",\n",
    "                                    \"learning_rate\": 1e-4, # hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, default=1e-3):w\n",
    "                                    \"epochs\": 50,\n",
    "                                    \"batch_size\": 27,\n",
    "                                    \"loss\": \"mean_absolute_error\",\n",
    "                                    \"metrics\": [\"mean_absolute_error\"],\n",
    "                                    \"warm_start\": True,\n",
    "                                    \"jit_compile\": False,\n",
    "\n",
    "                                }\n",
    "                            }\n",
    "    \n",
    "\n",
    "model_settings = {\n",
    "    \"models\": [\"linear\", \"lasso\", \"dtree\", \"knn\", \"rforest\", \"nn_new\"],\n",
    "    \"nn_new\": [structural_hyperparameters, optimizer_hyperparameters],\n",
    "}\n",
    "\n",
    "# print(nnHyperModel(structural_hyperparameters=model_settings[\"nn_new\"][0],\n",
    "#                    optimizer_hyperparameters=model_settings[\"nn_new\"][1]).loss)\n",
    "\n",
    "tuning = mai.Tuning(data=data, model_settings=model_settings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e37e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Checking Initialization--\n",
      "opt =  None\n",
      "loss =  None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nnHyperModel(structural_hyperparameters=model_settings[\"nn_new\"][0],\n",
    "                   optimizer_hyperparameters=model_settings[\"nn_new\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46cc071",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0474815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if model is initialized\n",
      "optimizer =  None\n",
      "loss =  None\n",
      "Tuning\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "100               |80                |Units\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 15:26:26.353296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 15:26:26.356754: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\", line 213, in _build_and_fit_model\n",
      "    model = self._try_build(hp)\n",
      "  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\", line 155, in _try_build\n",
      "    model = self._build_hypermodel(hp)\n",
      "  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\", line 146, in _build_hypermodel\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"/home/connorcr/NERS491/pyMAISE/pyMAISE/methods/_nn_general.py\", line 183, in build\n",
      "    model.compile(optimizer= self._optimizer,\n",
      "  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras/optimizers/__init__.py\", line 318, in get\n",
      "    raise ValueError(\n",
      "ValueError: Could not interpret optimizer identifier: None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures excceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\", line 213, in _build_and_fit_model\n    model = self._try_build(hp)\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\", line 155, in _try_build\n    model = self._build_hypermodel(hp)\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\", line 146, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n  File \"/home/connorcr/NERS491/pyMAISE/pyMAISE/methods/_nn_general.py\", line 183, in build\n    model.compile(optimizer= self._optimizer,\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras/optimizers/__init__.py\", line 318, in get\n    raise ValueError(\nValueError: Could not interpret optimizer identifier: None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bayesian_search_configs \u001b[38;5;241m=\u001b[39m \u001b[43mtuning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbayesian_search_hypermodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_hps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callback_loss = 'val_loss'\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NERS491/pyMAISE/pyMAISE/tuning.py:396\u001b[0m, in \u001b[0;36mTuning.bayesian_search_hypermodel\u001b[0;34m(self, objective, max_trials, directory, project_name, num_folds, shuffle)\u001b[0m\n\u001b[1;32m    393\u001b[0m     ytrain_fold, ytest_fold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ytrain\u001b[38;5;241m.\u001b[39miloc[train_index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ytrain\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# hp tuning on kFolds\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxtrain_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m                 \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mytrain_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnn_new\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnn_new\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxtest_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytest_fold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# callbacks=[tf.keras.callbacks.EarlyStopping(monitor=callback_loss, patience=3, restore_best_weights=True)]\u001b[39;49;00m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# Get Hyperparameters\u001b[39;00m\n\u001b[1;32m    405\u001b[0m best_hps \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py:231\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py:335\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# Display needs the updated trial scored by the Oracle.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display\u001b[38;5;241m.\u001b[39mon_trial_end(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/oracle.py:107\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    106\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[0;32m--> 107\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[1;32m    109\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/oracle.py:434\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[0;32m--> 434\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_consecutive_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/oracle.py:386\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures excceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;241m+\u001b[39m trial\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    390\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures excceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\", line 213, in _build_and_fit_model\n    model = self._try_build(hp)\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\", line 155, in _try_build\n    model = self._build_hypermodel(hp)\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py\", line 146, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n  File \"/home/connorcr/NERS491/pyMAISE/pyMAISE/methods/_nn_general.py\", line 183, in build\n    model.compile(optimizer= self._optimizer,\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/connorcr/.local/lib/python3.8/site-packages/keras/optimizers/__init__.py\", line 318, in get\n    raise ValueError(\nValueError: Could not interpret optimizer identifier: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bayesian_search = tuning.bayesian_search_hypermodel(\n",
    "    objective = 'val_loss',\n",
    "    max_trials=10,\n",
    "    directory= \"./\",\n",
    "    project_name = \"best_hps\",\n",
    "    num_folds=5,\n",
    "    shuffle= True,\n",
    "    # callback_loss = 'val_loss'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2770ed13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b906b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b0b523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
